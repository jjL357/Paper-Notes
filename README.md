# Paper-Notes

This repository contains my personal notes on multiple papers, combining summaries, explanations, detailed notes, and slide presentations (PPT), along with links to the original papers and related GitHub repositories. Suitable for self-study or as a reference for others.

## KV Cache

CAKE_CASCADING AND ADAPTIVE KV CACHE
MInference1.0-AcceleratingPre-fillingfor
MMINFERENCE_ACCELERATING PRE-FILLING FOR LONG-CONTEXT VLMs VIA MODALITY-AWARE PERMUTATION SPARSE ATTENTION
HashAttention - Semantic Sparsity for Faster Inference - ICML'25
APE - Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding - ICLR'25
XAttention - Block Sparse Attention with Antidiagonal Scoring - ICML'25
CacheBlend - Fast Large Language Model Serving for RAGwithCached Knowledge Fusion - Eurosys'25
CacheGen - KV Cache Compression and Streaming for Fast Large - SIGCOMM'24

## MLLM

γ−MOD: EXPLORING MIXTURE-OF-DEPTH ADAPTATION FOR MULTIMODAL LARGE LANGUAGE MODELS
VoCo-LLaMA_Towards Vision Compression with Large Language Models
CoreMatching - Co-adaptive Sparse Inference Framework for Comprehensive Acceleration of Vision Language Model - ICML'25
VisionZip_Longer is Better but Not Necessary in Vision Language Models
DivPrune - Diversity-based Visual Token Pruning for Large Multimodal Models - CVPR'25
DyCoke - DynamicCompression of Tokens for Fast Video Large Language Models - CVPR'25
TimeChat-Online - 80% Visual Tokens are Naturally Redundant in Streaming Videos - MM'25
## Quantization

ResQ - Mixed-Precision Quantization of Large Language Models with Low-Rank Residuals - ICML'25
SageAttention3 - Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training
SageAttention
SliM-LLM_Salience-Driven Mixed-Precision Quatization for Large Language Models
Optimizing Large Language Model Training Using FP4 Quantization - ICML'25

## Recommendation System

## Speculative Decoding

PEARL_PARALLEL_SPECULATIVE_DECODING_WITH_ADAPTIVE_DRAFT_LENGTH      
MAGICDEC - BREAKING THE LATENCY-THROUGHPUT TRADEOFF FOR LONG CONTEXT GENERATION WITH SPECULATIVE DECODING - ICLR'25
## System

NEO - Saving GPU Memory Crisis with CPU Offloading for Online LLM Inference - MLsys'25
T-MAC eurosys

## Technical Report

Llama4
