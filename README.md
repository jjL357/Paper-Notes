# Paper-Notes

This repository contains my personal notes on multiple papers, combining summaries, explanations, detailed notes, and slide presentations (PPT), along with links to the original papers and related GitHub repositories. Suitable for self-study or as a reference for others.

### **KV Cache**
*   **APE - Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding** [ICLR'25] [[paper]](https://arxiv.org/abs/2502.05431) [[github]](https://github.com/Infini-AI-Lab/APE))[[slide]]()
*   **CacheBlend - Fast Large Language Model Serving for RAGwithCached Knowledge Fusion** [Eurosys'25] [[paper]](https://arxiv.org/abs/2406.01416) [[github]](https://github.com/RICE-EIC/CacheBlend)
*   **CacheGen - KV Cache Compression and Streaming for Fast Large** [SIGCOMM'24] [[paper]](https://arxiv.org/abs/2312.06569) [[github]](https://github.com/hpca-ucr/CacheGen)
*   **CAKE - Cascading and Adaptive KV Cache for Efficient LLM Inference** [[paper]](https://arxiv.org/abs/2405.08533) [[github]](https://github.com/yujia-gao/CAKE)
*   **HashAttention - Semantic Sparsity for Faster Inference** [ICML'25] [[paper]](https://arxiv.org/abs/2406.01736) [[github]](https://github.com/HazyResearch/HashAttention)
*   **M-Inference - Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention** [[paper]](https://arxiv.org/abs/2405.03456)
*   **MInference - Accelerating Pre-filling for Long-Context Large Language Models** [[paper]](https://arxiv.org/abs/2404.00448) [[github]](https://github.com/Infini-AI/MInference)
*   **XAttention - Block Sparse Attention with Antidiagonal Scoring** [ICML'25] [[paper]](https://arxiv.org/abs/2406.05083) [[github]](https://github.com/HazyResearch/X-Attention)

### **MLLM**
*   **CoreMatching - Co-adaptive Sparse Inference Framework for Comprehensive Acceleration of Vision Language Model** [ICML'25] [[paper]](https://arxiv.org/abs/2406.04029)
*   **DivPrune - Diversity-based Visual Token Pruning for Large Multimodal Models** [CVPR'25] [[paper]](https://arxiv.org/abs/2405.19635) [[github]](https://github.com/Fuxin-VI/DivPrune)
*   **DyCoke - DynamicCompression of Tokens for Fast Video Large Language Models** [CVPR'25] [[paper]](https://arxiv.org/abs/2406.02396)
*   **TimeChat-Online - 80% Visual Tokens are Naturally Redundant in Streaming Videos** [MM'25] [[paper]](https://arxiv.org/abs/2406.06233) [[github]](https://github.com/showlab/TimeChat)
*   **VisionZip - Longer is Better but Not Necessary in Vision Language Models** [[paper]](https://arxiv.org/abs/2405.10183)
*   **VoCo-LLaMA - Towards Vision Compression with Large Language Models** [[paper]](https://arxiv.org/abs/2405.05061)
*   **Î³-MOD - Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models** [[paper]](https://arxiv.org/abs/2405.15132)

### **Quantization**
*   **Optimizing Large Language Model Training Using FP4 Quantization** [ICML'25] [[paper]](https://arxiv.org/abs/2406.00835)
*   **ResQ - Mixed-Precision Quantization of Large Language Models with Low-Rank Residuals** [ICML'25] [[paper]](https://arxiv.org/abs/2406.01239)
*   **SageAttention** [[paper]](https://arxiv.org/abs/2402.09613)
*   **SageAttention3 - Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training** [[paper]](https://arxiv.org/abs/2406.02543)
*   **SliM-LLM - Salience-Driven Mixed-Precision Quantization for Large Language Models** [[paper]](https://arxiv.org/abs/2405.16362) [[github]](https://github.com/aritra-bhowmick/SliM-LLM)

### **Recommendation System**
*   **MAGICDEC - Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding** [ICLR'25] [[paper]](https://arxiv.org/abs/2405.15051) [[github]](https://github.com/princeton-nlp/MAGICDEC)
*   **PEARL - Parallel Speculative Decoding with Adaptive Draft Length** [[paper]](https://arxiv.org/abs/2405.07086)

### **System**
*   **NEO - Saving GPU Memory Crisis with CPU Offloading for Online LLM Inference** [MLsys'25] [[paper]](https://arxiv.org/abs/2405.19598)
*   **T-MAC - A Transport Protocol for Remote-Memory-Access-based LLM Inference** [EuroSys] [[paper]](https://dl.acm.org/doi/10.1145/3622645.3644265)

### **Technical Report**
*   **Llama 4** [[paper]](https://arxiv.org/abs/2404.17511)
