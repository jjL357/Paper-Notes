# Paper-Notes

This repository contains my personal notes on papers, combining summaries, explanations, detailed notes, and slide presentations (PPT), along with links to the original papers and related GitHub repositories. Suitable for self-study or as a reference for others.

### **KV Cache**
*   **APE - Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding** [ICLR'25] [[paper]](https://arxiv.org/abs/2502.05431) [[github]](https://github.com/Infini-AI-Lab/APE)
*   **CacheBlend - Fast Large Language Model Serving for RAGwithCached Knowledge Fusion** [Eurosys'25] [[paper]](https://arxiv.org/abs/2405.16444) 
*   **CacheGen: Fast Context Loading for Language Model Applications via KV Cache Streaming** [SIGCOMM'24] [[paper]](https://arxiv.org/pdf/2310.07240.pdf) [[github]](https://github.com/UChi-JCL/CacheGen)
*   **CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences** [ICLR'25] [[paper]](https://arxiv.org/html/2503.12491) [[github]](https://github.com/antgroup/cakekv)
*   **HashAttention - Semantic Sparsity for Faster Inference** [ICML'25] [[paper]](https://arxiv.org/abs/2412.14468) [[github]](https://github.com/xAlg-ai/HashAttention-1.0)
*   **MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention** [NeurIPS'24 Spotlight, ICLR'25] [[paper]](https://arxiv.org/abs/2405.03456) [[github]](https://github.com/microsoft/MInference)
*   **MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention** [[paper]](https://arxiv.org/abs/2504.16083) [[github]](https://github.com/microsoft/MInference)
*   **XAttention - Block Sparse Attention with Antidiagonal Scoring** [ICML'25] [[paper]](https://arxiv.org/abs/2503.16428) [[github]](https://github.com/mit-han-lab/x-attention)

### **MLLM**
*   **CoreMatching - Co-adaptive Sparse Inference Framework for Comprehensive Acceleration of Vision Language Model** [ICML'25] [[paper]](https://arxiv.org/abs/2406.04029)
*   **DivPrune - Diversity-based Visual Token Pruning for Large Multimodal Models** [CVPR'25] [[paper]](https://arxiv.org/abs/2405.19635) [[github]](https://github.com/Fuxin-VI/DivPrune)
*   **DyCoke - DynamicCompression of Tokens for Fast Video Large Language Models** [CVPR'25] [[paper]](https://arxiv.org/abs/2406.02396)
*   **TimeChat-Online - 80% Visual Tokens are Naturally Redundant in Streaming Videos** [MM'25] [[paper]](https://arxiv.org/abs/2406.06233) [[github]](https://github.com/showlab/TimeChat)
*   **VisionZip - Longer is Better but Not Necessary in Vision Language Models** [[paper]](https://arxiv.org/abs/2405.10183)
*   **VoCo-LLaMA - Towards Vision Compression with Large Language Models** [[paper]](https://arxiv.org/abs/2405.05061)
*   **Î³-MOD - Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models** [[paper]](https://arxiv.org/abs/2405.15132)

### **Quantization**
*   **Optimizing Large Language Model Training Using FP4 Quantization** [ICML'25] [[paper]](https://arxiv.org/abs/2501.17116)
*   **ResQ - Mixed-Precision Quantization of Large Language Models with Low-Rank Residuals** [ICML'25] [[paper]](https://arxiv.org/abs/2412.14363) [[github]](https://github.com/utkarsh-dmx/project-resq)
*   **SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration** [ICLR'25] [[paper]](https://arxiv.org/abs/2410.02367) [[github]](https://github.com/thu-ml/SageAttention)
*   **SageAttention3 - Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training** [[paper]](https://arxiv.org/abs/2505.11594) [[github]](https://github.com/thu-ml/SageAttention)
*   **SliM-LLM - Salience-Driven Mixed-Precision Quantization for Large Language Models** [ICML'25] [[paper]](https://arxiv.org/abs/2405.14917) [[github]](https://github.com/Aaronhuang-778/SliM-LLM)

### **Recommendation System**

### **Speculative Decoding**
*   **MAGICDEC - Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding** [ICLR'25] [[paper]](https://arxiv.org/abs/2408.11049) [[github]](https://github.com/Infini-AI-Lab/MagicDec/)
*   **PEARL - Parallel Speculative Decoding with Adaptive Draft Length** [ICLR'25] [[paper]](https://arxiv.org/abs/2408.11850)

### **System**
*   **NEO - Saving GPU Memory Crisis with CPU Offloading for Online LLM Inference** [MLsys'25] [[paper]](https://arxiv.org/abs/2411.01142) [[github]](https://github.com/NEO-MLSys25/NEO)
*   **T-MAC - A Transport Protocol for Remote-Memory-Access-based LLM Inference** [EuroSys'25] [[paper]](https://arxiv.org/abs/2407.00088) [[github]](https://github.com/microsoft/T-MAC)
### **Technical Report**
*   **Llama 4** [[blog]](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)
